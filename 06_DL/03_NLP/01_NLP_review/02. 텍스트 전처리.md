0825_NLP

## 02. 텍스트 전처리

word Tokenizaition

1. 불용어 제거
2. 영어 단어 소문자화
3. 길이가 짧은 단어 제거



Cleaning(정제) : 불필요한 데이터를 제거하는 일

- 불용어 제거 (I. at, for, by, at, 은, 는, 이, 가)

Normalization(정규화) : 같은 의미를 갖고 있다면 하나로 통일하여 복잡도를 줄임.

- 정규 표현식 or `solynlp.normalizer` 의 `emotion_normalize` 함수 사용



**Integer Encoding (정수 인코딩)** :star: 

- 토큰화 수행한 후 각 단어에 <u>중복되지 않게</u> **고유한 정수 부여**
- -> 단어 집합 (Vocabulary)



Padding

- 정수 인코딩 후 길이가 서로 다르므로 가상의 단어(0) 를 추가하여 <u>길이를 맞춤</u> -> 그 후 병렬 연산



Vectorization : One-Hot encoding

- 전체 단어 집합의 크기(중복되지 않은 단어들의 개수)를 **벡터**의 차원을 가짐
- 정수 인덱스 부여 - **해당 인덱스는 1**, 나머지는 0



Vectorization : Document Term Matrix, DTM

- 벡터가 단어 집합의 크기를 가지고, 대부분의 원소가 0
- 고유한 정수 인덱스를 가지고, 해당 단어가 **등장 횟수**를 해당 인덱스 값으로 가짐
- `Counter()`





Integer encoding (ver. Python)

1. 빈도수가 높은 순서대로 정렬

   ```python
   # 빈도수가 높은 순서대로 정렬
   vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)
   print(vocab_sorted)
   ```

   

2. 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여

   ```python
   # 이제 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여합니다.
   word2idx = {}
   i=0
   for (word, frequency) in vocab_sorted :
       if frequency > 1 : # 정제(Cleaning) 챕터에서 언급했듯이 빈도수가 적은 단어는 제외한다.
           i = i+1
           word2idx[word] = i
   print(word2idx)
   ```

   - 인덱스가 1부터 시작하도록 함



3. 상위 5개 단어만 사용할 때

   ```python
   vocab_size = 5
   words_frequency = [w for w,c in word2idx.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거
   for w in words_frequency:
       del word2idx[w] # 해당 단어에 대한 인덱스 정보를 삭제
       
   print(word2idx)
   ```

   

4. word2idx를 사용해서 단어 토큰화 된 상태로 저장된 sentences에 있는 각 단어를 정수로 encoding

   - 단어 집합에 없는 단어 : `OOV`로 인코딩

   ```python
   word2idx['OOV'] = len(word2idx) + 1
   print(word2idx)
   ```

   ```python
   encoded = []
   for s in sentences:
       temp = []
       for w in s:
           try:
               temp.append(word2idx[w])
           except KeyError:
               temp.append(word2idx['OOV'])
       encoded.append(temp)
   encoded
   ```

   

Integer encoding (ver. Tensorflow)

1. 정수 인코딩 수행하는 전처리 도구 `keras.preprocessing.text.Tokenizer`

   ```python
   from tensorflow.keras.preprocessing.text import Tokenizer
   ```

   ```python
   tokenizer = Tokenizer()
   tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.
   ```

   - `fit_on_texts()` : 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스 부여



2.  각 단어 인덱스 확인 `word_index`

   ```python
   print(tokenizer.word_index) # 각 단어에 인덱스가 어떻게 부여됐는지 확인 (word_index)
   ```

   

3. 각 단어 카운트 횟수 확인 `word_counts`

   ```python
   print(tokenizer.word_counts)
   ```

   

4. 빈도수가 가장 높은 단어 n개 사용 `most_common()`

   - `tokenizer = Tokenizer(num_words=숫자)` 빈도수가 높은 상위 몇 개의 단어만 사용

   ```python
   vocab_size = 5
   tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용
   tokenizer.fit_on_texts(sentences)
   ```

   - 빈도수 5개만 사용
   - num_words에서 +1 하는 이유는 num_words가 0부터 시작하기 때문에 1부터 시작하도록 하게 하기 위해서 임
   - 후에 패딩(padding) 작업할 때 케라스 토크나이저가 숫자 0까지 단어 집합의 크기로 산정



5. 각 단어를 인덱스로 변환

   ```python
   print(tokenizer.texts_to_sequences(sentences))
   ```

   

6. 단어 집합에 없는 단어들을 OOV로 간주하여 보존

   ```python
   vocab_size = 5
   # 빈도수 상위 5개 단어만 사용. 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2
   tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')
   tokenizer.fit_on_texts(sentences)
   print(tokenizer.texts_to_sequences(sentences))
   ```

   ```python
   print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))
   ```

   - OOV 인덱스는 1

   ```python
   print(tokenizer.texts_to_sequences(sentences))
   ```

   

Padding

- 정수 시퀀스로 변환된 각 문장의 길이를 동일하게 맞춰줌
- 병렬 연산을 하기 위함



Padding (ver. Numpy)

```python
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
```



1. 가장 길이가 긴 문장의 길이 계산

   ```python
   max_len = max(len(item) for item in encoded)
   print(max_len)
   ```



2. 0으로 채움

   ```python
   for item in encoded:                # for each item in the list
       while len(item) < max_len:   # while the item length is smaller than 3
           item.append(0)
   
   padded_np = np.array(encoded)
   ```

   - 'PAD' 라는 가상의 단어를 사용해서 그 단어를 0번 단어라고 정의
   - 7보다 짧은 문장에 숫자 0을 채워서 길이 7로 맞춤 (뒤에 0으로 채움)



Padding (ver.Tensorflow)

1. 패딩 하기 전으로 초기화

   ```python
   padded = pad_sequences(encoded)
   ```

2. 0으로 채움

   - default는 앞에 0으로 채움
   - `padding = 'post'` 뒤에 0으로 채움

   ```python
   padded = pad_sequences(encoded, padding = 'post')
   ```

   - 문서 길이에 제한을 두고 패딩
     - `maxlen = `

   ```PYTHON
   padded = pad_sequences(encoded, padding = 'post', maxlen = 5)
   ```

   - 다른 숫자로 채우고 싶을 때

   ```python
   last_value = len(tokenizer.word_index) + 1
   print(last_value)
   ```

   - 단어 집합 크기에 +1 한 경우

   ```python
   padded = pad_sequences(encoded, padding = 'post', value = last_value)
   ```

   

One-Hot Encoding

- 정수 인덱스 -> 벡터화



One-Hot Encoding (ver. Python)

1. `morphs`

   ```python
   token = okt.morphs("나는 자연어 처리를 배운다")  
   print(token)
   ```

2. 정수 인덱스

   ```python
   word2idx={}
   for voca in token:
        if voca not in word2idx.keys():
          word2idx[voca] = len(word2idx)
   print(word2idx)
   ```

3. 원핫 인코딩 :star:

   ```python
   def one_hot_encoding(word, word2idx):
          one_hot_vector = [0]*(len(word2idx))
          index = word2idx[word]
          one_hot_vector[index]=1
          return one_hot_vector
   ```



One-Hot Encoding (ver. Tensorflow)

1. 정수 인덱스

   ```python
   from tensorflow.keras.utils import to_categorical
   ```

   ```python
   tokenizer = Tokenizer()
   tokenizer.fit_on_texts([text])
   print(tokenizer.word_index) # 각 단어에 대한 인코딩 결과 출력.
   ```

2. 정수 시퀀스로 변환 `texts_to_sequences()`

   ```python
   sub_text = "점심 먹으러 갈래 메뉴는 햄버거 최고야"
   encoded = tokenizer.texts_to_sequences([sub_text])[0]
   print(encoded)
   ```

3. 원핫 인코딩 `to_categorical()`

   ```python
   one_hot = to_categorical(encoded)
   print(one_hot)
   ```

   

