

# 09. 워드 임베딩

## 01) 워드 임베딩(Word Embedding)

### 1. 희소 표현 (Sparse Representation)

- 벡터 또는 행렬(matrix) 값이 대부분 0으로 표현되는 방법
- 원-핫 벡터 = 희소 벡터(sparse vector)



- 희소 벡터의 문제점 : 단어의 개수가 늘어나면 벡터의 차원이 한없이 커짐

  - 공간적 낭비

  - 단어 벡터 간 유의미한 유사도를 계산할 수 없음



### 2. 밀집 표현 (Dense Representation)

- 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤
- 0과 1만 가진 값이 아닌 실수값을 가짐
- ex) 밀집 표현의 차원이 128이라면, 모든 단어의 벡터 표현은 128로 바뀌면서 모든 값이 실수



### 3. 워드 임베딩 (Word Embedding)

- 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법
- 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 **임베딩 벡터(embedding vector)**라고 함



| _         | 원-핫 벡터                | 임베딩 벡터              |
| --------- | ------------------------- | ------------------------ |
| 차원      | 고차원 (단어 집합의 크기) | 저차원                   |
| 다른 표현 | 희소 벡터의 일종          | 밀집 벡터의 일종         |
| 표현 방법 | 수동                      | 훈련 데이터로부터 학습함 |
| 값의 타입 | 1과 0                     | 실수                     |



## 02) 워드투벡터 (Word2Vec)

- 원-핫 벡터는 단어 벡터 간 유의미한 유사도를 계산할 수 없음

  -> **단어의 의미를 수치화** 하여 단어 벡터 간 유의미한 유사도를 구할 수 있음



### 1. 희소 표현 (Sparse Representation)

- 희소 표현은 각 단어 벡터간 유의미한 유사성을 표현할 수 없다는 단점이 있음
- 대안으로 <u>단어의 의미를 다차원 공간에 벡터화</u>하는 방법을 사용 :point_right: **분산 표현(distributed representation)**
- 분산 표현을 이용하여 단어 간 의미적 유사성을 벡터화하는 작업을 워드 임베딩(embedding)



### 2. 분산 표현(Distributed Representation)

- 분산 표현 방법은 기본적으로 분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법
- **'비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다'**
- 분산 표현은 분포 가설을 이용해서 텍스트를 학습하고, 단어의 의미를 벡터의 여러 차원에 분산하여 표현함
- 원-핫 벡터처럼 벡터의 차원이 단어 집합(vocabulary)의 크기일 필요가 없어서 **저차원**으로 줄어듦
- ==**저차원에 단어의 의미를 여러 차원에다가 분산하여 표현 :point_right: 단어 벡터 간 유의미한 유사도를 계산할 수 있음**==
- 대표적인 학습 방법 : Word2Vec
  - 학습 방식 : CBOW, Skip-Gram



## 3. CBOW (Continuous Bag of Words)

- CBOW : 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측
- Skip-Gram : 중간에 있는 단어들을 입력으로 주변 단어들을 예측



#### 개념

-----

- 예문 - "The fat cat sat on the mat"

  - ['The', 'fat', 'cat', 'on', 'the', 'mat'] => 'sat' 예측
  - 'sat' : 중심 단어(center word), 나머지 : 주변 단어(context word)

- 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지를 결정하는데 이 범위를 **윈도우(window)**라고 함

- 윈도우 크기가 정해지면 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습을 위한 데이터 셋을 만드는데 이 방법을 **슬라이딩 윈도우(sliding window)**라고 함

  ![image-20220914134106547](09.%20%EC%9B%8C%EB%93%9C%20%EC%9E%84%EB%B2%A0%EB%94%A9-imgaes/image-20220914134106547.png)

  > window 크기가 2일 때

- 입력층(Input Layer) - 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터

- 출력층(Output Layer) - 예측하고자 하는 중간 단어의 원-핫 벡터가 레이블로서 필요

- ![image-20220914134229537](09.%20%EC%9B%8C%EB%93%9C%20%EC%9E%84%EB%B2%A0%EB%94%A9-imgaes/image-20220914134229537.png)

  > CBOW의 인공 신경망
  >
  > - 은닉층이 1개인 얕은 신경망(shallow neural network)

- **Word2Vec의 은닉층은** 일반적인 은닉층과는 달리 활성화 함수가 존재하지 않고, 룩업 테이블이라는 연산을 담당하는 **투사층(Projection Layer)**

- Word2Vec의 은닉층 = 룩업 테이블이라는 연산을 담당하는 투사층(projection layer)



#### 동작 메커니즘

-----

1. 투사층의 크기가 M

   - 투사층의 크기 M = 임베딩하고 난 벡터의 차원

2. 입력층과 투사층 사이의 가중치 W는 V x M 행렬

    투사층에서 출력층 사이의 가중치 W' 는 M x V 행렬

   - V = 단어 집합의 크기 (= 원-핫 벡터의 차원)

   - M = 투사층 크기 (= 임베딩하고 난 벡터의 차원)

   - 두 행렬은 전치(transpose) 관계가 아니라 서로 다른 행렬

     ![image-20220914135151666](09.%20%EC%9B%8C%EB%93%9C%20%EC%9E%84%EB%B2%A0%EB%94%A9-imgaes/image-20220914135151666.png)