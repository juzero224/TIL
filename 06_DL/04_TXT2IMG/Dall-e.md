# Dall-e



## 1. Introduction

- DALL-E : Zero-Shot Text-to-Image Generation 

  - 120ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ Transformer ê¸°ë°˜ì˜ GPT-3ì˜ í™•ì¥ í˜•íƒœì˜ ëª¨ë¸ë¡œ 2.5ì–µ ê°œì˜ ë°ì´í„°(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€) ìŒìœ¼ë¡œ í•™ìŠµ

  - Computer Visionê³¼ NLP ê¸°ìˆ ì„ ê²°í•©í•˜ì—¬ ë§Œë“  Text-to-Image Taskë¥¼ Auto-regressiveí•˜ê²Œ ëª¨ë¸ë§

  - ë³µì¡í•œ ì•„í‚¤í…ì²˜ë‚˜ ì¶”ê°€ì ì¸ ë ˆì´ë¸” ì •ë³´ ì—†ì´ ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„

  - ì˜ í•™ìŠµëœ DALL-Eë¥¼ ì´ìš©í•˜ì—¬, zero-shot ìƒí™©ì—ì„œë„ ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„

    - zero-shot : íŠ¹ì •í•œ ë°ì´í„°ì…‹ì´ ì¡´ì¬í•  ë•Œ í•™ìŠµ ë°ì´í„°ì…‹ì—ëŠ” ì ‘ê·¼í•˜ì§€ ëª»í•˜ëŠ” ìƒí™©ì—ì„œ í•œ ë²ˆë„ ë³¸ ì  ì—†ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒ

  - ì‚¬ë¬¼ì„ ì˜ì¸í™”í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ê³ , ì„œë¡œ ê´€ë ¨ì´ ì—†ëŠ” ë‘ ê°œì˜ ì»¨ì…‰ì„ í•©ì¹˜ëŠ” ê²ƒ ë˜í•œ ê°€ëŠ¥í•¨

    ![image-20220904213317605](Dall-e-imgaes/image-20220904213317605.png)



## 2. Background

1) ì„ í–‰ ì—°êµ¬

   - 2015ë…„ DRAW Generative modelì„ ì‹œì‘ìœ¼ë¡œ ë³¸ê²©ì ìœ¼ë¡œ ì—°êµ¬ë¨

     -> Image caption ì¡°ê±´ ì•„ë˜ Novel visual scene ìƒì„±

   - 2016ë…„ Recurrent VAEë¥¼ ëŒ€ì‹ í•˜ì—¬ GAN ì´ìš©

     -> Image ì •í™•ë„ í–¥ìƒ, ì¤€ìˆ˜í•œ Zero-shot ì¼ë°˜í™” ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤Œ

   - StackGAN : Multi-scale generate ì‚¬ìš© (2017)

   - AttnGAN : Integrating attention and auxiliary losses (2018)

   - Text ì™¸ ì¶”ê°€ì ì¸ ì¡°ê±´ í™œìš© : Object location, Pre-generated semantic layout, Mouse trace

   - Pretrained-cross-modal masked language  model í™œìš©í•œ ì—°êµ¬

   - í•œê³„ì  : ë¬¼ì²´ ì™œê³¡, ì‹¤ì¬í•  ìˆ˜ ì—†ëŠ” ë¬¼ì²´ì˜ ìœ„ì¹˜, ë°°ê²½ê³¼ ì–´ìš°ëŸ¬ì§€ì§€ ëª»í•˜ëŠ” ë¬¼ì²´

     -> Suggest text to image generative by using large-scale dataset

     ![image-20220903144953648](C:/Users/yes47/AppData/Roaming/Typora/typora-user-images/image-20220903144953648.png)

2. Transformer : Attention is All You Need (NIPS 2017)

   - ì „í†µì ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” ë§ˆì§€ë§‰ ì¸ì½”ë” ë ˆì´ì–´ì˜ ì¶œë ¥ì´ ëª¨ë“  ë””ì½”ë” ë ˆì´ì–´ì— ì…ë ¥ë¨
     - ë ˆì´ì–´ ê°œìˆ˜ê°€ 4ê°œì¼ ë•Œì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŒ
     - ex. ê¸°ê³„ë²ˆì—­
       - ì…ë ¥ : "I am a teacher" -> encoderë¥¼ ê±°ì³ì„œ ì ì ˆí•œ semantic informationì„ ì¶”ì¶œ -> decoderì—ì„œ ì´ëŸ¬í•œ ì •ë³´ì˜ attentionì„ ìˆ˜í–‰í•´ì„œ ê²°ê³¼ì ìœ¼ë¡œ ì–´ë–¤ í† í°ì´ ë‚˜ì™€ì•¼í•˜ëŠ”ì§€ í•˜ë‚˜ì”© ê²°ê³¼ë¥¼ ì•Œë ¤ì¤Œ

   ![image-20220904213525602](Dall-e-imgaes/image-20220904213525602.png)



3. GPT-2 (<- Dall-e 1) (OpenAI 2019)

   - íŠ¸ëœìŠ¤í¬ë¨¸ì˜ decoder ê¸°ë°˜ì˜ ì•„í‚¤í…ì²˜ë¡œ, ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ë¡œ í•™ìŠµëœ ëŒ€ìš©ëŸ‰ ì–¸ì–´ ëª¨ë¸
     - ê¸°ê³„ ë²ˆì—­ ëª©ì ì´ ì•„ë‹ˆë¯€ë¡œ, decoderë§Œ ì‚¬ìš©í•´ë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìŒ
     - íŠ¹ì •í•œ ë¬¸ì¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ìŒìœ¼ë¡œ ë“±ì¥í•  ë‹¨ì–´ê°€ ë¬´ì—‡ì¸ì§€ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹
   - autoregressively models the text tokens as a single steam of data
   - í† í°(ë‹¨ì–´) sequenceë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ìœ¼ë©´ í•˜ë‚˜ì˜ í† í°ì´ ì¶œë ¥ë˜ë©°, ì´ë¥¼ ë‹¤ì‹œ ì…ë ¥ sequenceì— ì¶”ê°€

   ![image-20220904214352269](Dall-e-imgaes/image-20220904214352269.png)



4. ì–¸ì–´ ëª¨ë¸ (Language Model)

   - ìƒì„± ëª¨ë¸

   - ì—¬ëŸ¬ í† í°ì— ëŒ€í•œ ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ í™•ë¥  ê°’ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘

   - $$
     Pr(x_1, x_2, ..., x_n)
     $$

     - x_1, x_2..., x_n : í† í°ì˜ sequence

   - í™•ë¥ ë¡ ì˜ chain rule ì´ìš©

     - ë² ì´ì¦ˆ ì •ë¦¬ í™•ì¥í•˜ì—¬ ë³µìˆ˜ì˜ ì‚¬ê±´ x1, x2... xnì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë‹¤ìŒì²˜ëŸ¼ ì“°ëŠ” ê²ƒ

     - $$
       Pr(x_1, x_2, ..., x_n) = Pr(x_1) * Pr(x_2|x_1) * Pr(x_3|x_1, x_2),...,*Pr(x_n|x_1,x_2,...,x_{n-1})\\
       = \prod_{i=1}^n Pr(x_i|x_1,x_2,...,x_{i-1})
       $$

   - ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ i+1ë²ˆì§€ì— ë‹¨ì–´ë¥¼ sampling (ì˜ˆì¸¡)í•¨ìœ¼ë¡œì¨ ë‹¤ìŒ ë‹¨ì–´ê°€ ë¬´ì—‡ì¸ì§€ ìƒì„±í•˜ëŠ” ê²ƒ

   - ì¶”ì¸¡í•  ë•ŒëŠ” í™•ë¥ ê°’ì´ ê°€ì¥ ë†’ì€ ê²ƒ ì„ íƒ
     $$
     \widehat{x}_{i+1} \sim f_\theta (f_{i+1}|x_1, x_2, ..., x_i)
     $$

   - ê·¸ í›„ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ë‹¤ìŒ ë‹¨ì–´ì— ë‹¤ì‹œ ë„£ì–´ì„œ ì˜ˆì¸¡
     $$
     \widehat{x}_{i+2} \sim f_\theta (f_{i+1}|x_1, x_2, ..., \widehat{x}_{i+1})
     $$

   - ì¤‘ì§€ ì¡°ê±´(eos í† í°(end of sequence)) ì´ ë‚˜ì˜¬ ë•Œ ê¹Œì§€ ë°˜ë³µí•˜ëŠ” ë“±ìœ¼ë¡œ ì „ì²´ ë¬¸ì¥ì„ ìƒì„±í•  ë•Œê¹Œì§€ ë°˜ë³µ

   - sampling ë°©ë²• : 'greedy' sampling, top-n sampling ë“±ì´ ìˆìŒ

   ![image-20220904220018211](Dall-e-imgaes/image-20220904220018211.png)

   - [ì—°ì‡„ ë²•ì¹™] P(S) = P(I) x P(am|I) x P(having|I am) x P(lunch|I am having)



5. ì˜¤í†  ì¸ì½”ë”(Auto-Encoder)

   - ë°ì´í„° ì¸ì½”ë”© (data encoding)ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬
     - í•™ìŠµí•  ë•ŒëŠ” <u>ì…ë ¥ ë°ì´í„°ì™€ ì¶œë ¥ ë°ì´í„°ë¥¼ ë™ì¼í•˜ê²Œ ì„¤ì •</u>
     - Encoder -> z (latent vecotr)ë¡œ ì••ì¶•ë˜ì—ˆë‹¤ê°€ -> Decoderì—ì„œ ë‹¤ì‹œ ë³µì›ë˜ëŠ” í˜•íƒœë¡œ í•™ìŠµ
   - ëª¨ë“  ì…ë ¥ ì´ë¯¸ì§€ëŠ” bottleneckì— í•´ë‹¹í•˜ëŠ” ì¤‘ê°„ **latent vector z**ë¡œ ë³€í™˜ë˜ì—ˆë‹¤ê°€ ë³µì›ë¨
     - ì…ë ¥ ì´ë¯¸ì§€ëŠ” ì••ì¶•ëœ ì •ë³´(latent code)ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŒ

   ![image-20220904220326877](Dall-e-imgaes/image-20220904220326877.png)

   - ì¼ë°˜ì ìœ¼ë¡œ í”½ì…€ ê³µê°„ì—ì„œ ë‘ ì´ë¯¸ì§€ë¥¼ ì„ í˜• ë³´ê°„í•˜ë©´ ì¤‘ê°„ ì´ë¯¸ì§€ì˜ sementic informationì´ ë¶€ìì—°ìŠ¤ëŸ¬ì›€

     - ```
       ì„ í˜• ë³´ê°„ë²•ì€ 1ì°¨ì› ì§ì„ ìƒì—ì„œ ë‘ ì ì˜ ê°’ì´ ì£¼ì–´ì¡Œì„ ë•Œ ê·¸ ì‚¬ì´ì˜ ê°’ì„ ì¶”ì •í•˜ê¸° ìœ„í•´ ì§ì„  ê±°ë¦¬ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ê³„ì‚°(ë¹„ë¡€ì‹)í•˜ëŠ” ë°©ë²•
       ```

   - ì ì¬ ê³µê°„ì˜ ë‘ ë²¡í„°ë¥¼ ì„ í˜• ë³´ê°„í•˜ë©´ í•™ìŠµëœ manifold ìœ„ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¯¸ì§€ ë³€í™˜ì´ ì´ë£¨ì–´ì§

     - ë‘ latent vector ì‚¬ì´ì—ì„œ í•™ìŠµëœ mainfold ìœ„ì—ì„œ linear interpolationë¥¼ ìˆ˜í–‰
     - ì˜¤í†  ì¸ì½”ë”ëŠ” <u>ì €ì°¨ì›ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì••ì¶•</u>í•˜ë¯€ë¡œ, data manifoldë¥¼ í•™ìŠµí•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰



6. VAE (Variational Auto-Encoder) 

   - VAEì˜ decoderëŠ” <span style="color:crimson">latent code</span>ê°€ <span style="color:Cornflowerblue">ì‚¬ì „ì— ì •í•´ ë†“ì€ ë¶„í¬(Gaussian ë“±)</span>ì„ ë”°ë¥¸ë‹¤ê³  ê°€ì •
   - meanê³¼ variationìœ¼ë¡œ êµ¬ì„±ëœ distributionì—ì„œ latent vectorë¥¼ samplingí•´ì„œ decoderì— ë“¤ì–´ê°”ì„ ë•Œ ì›ë³¸ ì´ë¯¸ì§€ê°€ ë³µì›ë  ìˆ˜ ìˆë„ë¡ í•¨

   ![image-20220904221858821](Dall-e-imgaes/image-20220904221858821.png)

   <img src="Dall-e-imgaes/image-20220904222119194.png" alt="image-20220904222119194" style="zoom: 50%;" />

   ![image-20220904222531210](Dall-e-imgaes/image-20220904222531210.png)

   - Reconstruction Term, Regularization Term ë‘ê°œë¡œ êµ¬ì„±í•´ì„œ roll boundë¥¼ ìµœëŒ€í™”
   - ì…ë ¥ ì´ë¯¸ì§€ xê°€ ë“¤ì–´ì™”ì„ ë–„ latent vectorì¸ zë¥¼ sampling í•˜ëŠ” ê²ƒì´ ëª©ì 
   - ê·¸ë˜ì„œ xê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ encoderì—ì„œ meanê³¼ varianceë¥¼ ë‚´ë³´ë‚´ê³  ì´ê²ƒì„ ë”°ë¥´ëŠ” latente vectorë¥¼ sampling í•˜ê¸° ìœ„í•´ ì ì ˆí•œ gradientë¥¼ êµ¬í•  ìˆ˜ ìˆë„ë¡ Re-parameterization Trickì„ ì‚¬ìš©
   - ë³„ë„ì˜ ê°€ìš°ì‹œì•ˆ  ë¶„í¬ì—ì„œ ëœë¤í•˜ê²Œ ì…ì‹¤ë¡  ë²¡í„°ë¥¼ ì¶”ì¶œí•´ì„œ ì´ê²ƒì— meanì„ ê³±í•´ì£¼ê³  variation ê°’ì„ ë”í•´ì¤˜ì„œ latent vector zë¥¼ êµ¬í•¨ ê·¸ê²ƒì´ decoderì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆë„ë¡ í•¨
   - **D_KL(Regularization Term)** ì€ q(z|x)ê°€ í‘œì¤€ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥¼ ìˆ˜ ìˆë„ë¡ ì œì•½ ì¡°ê±´ì„ ê±¸ì–´ì£¼ëŠ” ê²ƒ
   - **E(Reconstruction Term)**ëŠ” xê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ latent vector zë¥¼ êµ¬í•œ ë’¤ ë‹¤ì‹œ ì›ë˜ ì´ë¯¸ì§€ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒ



7. VQ-VAE : Vector Quantised-Variational AutoEncoder (NIPS 2017)

   - The encoder network outputs **discrete**, rather than continuous, codes; and the prior is **learnt** rather than static (ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ëŠ” ì—°ì†ì ì¸ ì½”ë“œê°€ ì•„ë‹Œ ì´ì‚°ì ì¸ ì½”ë“œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì‚¬ì „ì€ ì •ì ì¸ ê²ƒë³´ë‹¤ í•™ìŠµëœ ê²ƒì…ë‹ˆë‹¤.)
   - CNNì„ ê±°ì¹œ ê²°ê³¼ë¥¼ H * Wê°œì´ ê·¸ë¦¬ë“œë¡œ ë‚˜ëˆ„ê³  (ê° ìœ„ì¹˜ë§ˆë‹¤ Dì°¨ì›), ê° ìœ„ì¹˜ë§ˆë‹¤ e_1ë¶€í„° e_kê¹Œì§€ ì¤‘ì—ì„œ ê°€ê¹Œìš´ 1ê°œë¡œ ë³€í™˜

   ![image-20220904223849481](Dall-e-imgaes/image-20220904223849481.png)

   - ì¸ì½”ë”© ìˆ˜í–‰ ê²°ê³¼ê°€ ìˆì„ë•Œ (z_e(x)) representationì´ ë˜ê³  ê°€ì¥ ê¹Œìš´ ì½”ë“œë¶ vectorë¥¼ ì°¾ì•„ì„œ (e_j) ê·¸ ì¸ë±ìŠ¤ë¥¼ kë¼ê³  í•´ì„œ kë²ˆì§¸ ì¸ë±ìŠ¤ì˜ ì½”ë“œë¶ vectorê°€ ì‹¤ì§ˆì ìœ¼ë¡œ decoderì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒ
   - lossëŠ” reconstruction lossëŠ” ì…ë ¥ ì´ë¯¸ì§€ xê°€ ìˆì„ ë•Œ  encoderì— ë„£ê³  quantizationì„ ìˆ˜í–‰í•´ì„œ ë‚˜ì˜¨ tensorë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ ë‹¤ì‹œ ì›ë³¸ ì´ë¯¸ì§€ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒ
   - VQ lossì™€ commitment loss í•¨ê»˜ ì‚¬ìš©í•´ì„œ codebookê³¼ encoder ì „ë°˜ì— ê±¸ì³ì„œ í•™ìŠµì´ ìˆ˜í–‰ë  ìˆ˜ ìˆë„ë¡ í•¨
     - ì´ë•Œ sgëŠ” stop gradient, vq lossì—ì„œ ì½”ë“œ ë¶ vectorì¸ eë§Œ ìˆ˜í–‰ë  ìˆ˜ ìˆë„ë¡ í•¨

   ![image-20220904224652090](Dall-e-imgaes/image-20220904224652090.png)



8. Generating Diverse High-Fidelity Images with VQ-VAE 2 (NIPS 2019)

   ![image-20220904224818572](Dall-e-imgaes/image-20220904224818572.png)

   - 



2. GPT-3 : Generative Pre-trained Transformer 3 (<- Dall-e 2)

   - Auto-regressive ë³‘í•©ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” unsupervised pretrained lauguage model

   - Model & ArchitectureëŠ” GPT-2ì™€ ë™ì¼

   - 15ì–µê°œ íŒŒë¼ë¯¸í„° -> 1750ì–µ ê°œì˜ parameter í™•ì¥

   - Full self-attention -> sparse self-attentionìœ¼ë¡œ ë³€ê²½

     ![image-20220903145124680](C:/Users/yes47/AppData/Roaming/Typora/typora-user-images/image-20220903145124680.png)



3. AttnGAN

   - Fine-grained text to image generative with attentional generative adversarial networks

     ![image-20220903145305208](C:/Users/yes47/AppData/Roaming/Typora/typora-user-images/image-20220903145305208.png)

## 3. Methodology

- ëª©í‘œ : "Attention is all you need"ì—ì„œ ì œì•ˆëœ Autoregressive Transformer í•™ìŠµ

  -> Text & Image tokenì„ Single streamìœ¼ë¡œ Modeling

- Issues

  1. Memory issue : ê³ í•´ìƒë„ì˜ Imageë¥¼ Pixel ë‹¨ìœ„ì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ì‚¬ìš©
  2. Short-range dependece : Likelihoodë¥¼ Object functionìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” Modelë“¤ì€ Pixel ê°„ì˜ Short-range dependenceë¥¼ ìš°ì„  (High-frequency detailì„ ìœ„í•´ Capacity ì‚¬ìš©)

- Solution : 2-Stage Training



---

### Overview

<img src="Dall-e-imgaes/image-20220903150249329.png" alt="image-20220903150249329" style="zoom: 67%;" />

- Satge1

  - Discrete VAEë¥¼ ì´ìš©í•˜ì—¬ 256 x 256 RGB Image

    -> 32 x 32 Image Tokenìœ¼ë¡œ ì••ì¶•

  - ê° Tokenì€ 8192ê°€ì§€ì˜ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ

  - Context size 192ë°°ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŒ (8 x 8 x 3)

- Stage2

  - ìµœëŒ€ 256 BPE-encoded text tokensì„ 32x32 Image tokenê³¼ Concatenate
  - Text & Image tokenì˜ ê²°í•© ë¶„í¬ Modelingí•˜ëŠ” Autoregressive transformer í•™ìŠµ



- ë¨¼ì € text tokenë“¤ì´ ìµœëŒ€ 256ê°œ ë“¤ì–´ê°€ê³ , ì´ì–´ì„œ image tokenë“¤ì´ ìµœëŒ€  1,024ê°œ ì…ë ¥ë  ìˆ˜ ìˆìŒ

- ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” textë§Œ ë„£ê±°ë‚˜ text + image(rectangular region)ë¥¼ ë„£ì–´ì„œ ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒ

  ![image-20220903150458591](C:/Users/yes47/AppData/Roaming/Typora/typora-user-images/image-20220903150458591.png)





- ![image-20220903150616761](Dall-e-imgaes/image-20220903150616761.png)

  - x : Images, y : Captions, z : Encoded RGB Image ì˜ token

- ![image-20220903150658070](Dall-e-imgaes/image-20220903150658070.png)

  - ![image-20220903150712226](Dall-e-imgaes/image-20220903150712226.png)

  - ğ‘ğœƒ : dVAE ë””ì½”ë” (ì´ë¯¸ì§€ í† í°ì„ í† ëŒ€ë¡œ ê²°ê³¼ ì´ë¯¸ì§€ ì˜ˆì¸¡)
  - q : dVAE ì¸ì½”ë” (ì…ë ¥ ì´ë¯¸ì§€ë¥¼ í† ëŒ€ë¡œ ì´ë¯¸ì§€ í† í° ì˜ˆì¸¡)
  - ğ‘ğœ“ : Transformer (í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ í† í°ì— ëŒ€í•œ joint distribution ì˜ˆì¸¡)



----

### DALLE-E í•™ìŠµ ê³¼ì •

- **two-stage** training procedure ì‚¬ìš©
  1. 256 x 256 ì´ë¯¸ì§€ë¥¼ 32 x 32 gridì˜ ì´ë¯¸ì§€ í† í°ë“¤ë¡œ ì••ì¶• (ê° í† í°ì€ 8,192ê°œì˜ code ì¤‘ 1ê°œë¡œ ë°°ì •)
     - ì´ë¡œì¨ <u>í° qulity ì†ì‹¤ ì—†ì´</u> transformerì˜ context sizeë¥¼ 8 x 8 x 3ë°°ë§Œí¼ ì ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŒ
     - í”½ì…€ í•˜ë‚˜ì”© ì—°ë‹¬ì•„ ìƒì„±í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ íš¨ìœ¨ì ì„
  2. 256ê°œì˜ BPE-encoded text token ë“¤ê³¼ 1,024ê°œì˜ image tokenë“¤ì´ ì—°ì†ì ìœ¼ë¡œ ì…ë ¥ë  ìˆ˜ ìˆìŒ
     - Autoregressive transformerë¥¼ í•™ìŠµí•˜ì—¬ text tokensê³¼ image tokensì˜ joint distributionì„ ëª¨ë¸ë§
     - ì˜ˆë¥¼ ë“¤ì–´ í”½ì…€ì„ ì¼ì¼ì´ í•˜ë‚˜ì”© ì—°ë‹¬ì•„ ì˜ˆì¸¡(ê³„ì‚°) í•˜ëŠ” ê²½ìš° í”½ì…€ ê°œìˆ˜ë§Œí¼ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•´ì•¼ í•˜ë¯€ë¡œ, ì—°ì‚°ì´ ë¹„íš¨ìœ¨ì  (ê¸´ sequence)



### DALL-E ë™ì‘ ì›ë¦¬

- ë¨¼ì € text tokenë“¤ì´ ìµœëŒ€ 256ê°œ ë“¤ì–´ê°€ê³ , ì´ì–´ì„œ image tokenë“¤ì´ ìµœëŒ€ 1,024ê°œ ì…ë ¥ë  ìˆ˜ ìˆìŒ

- ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” textë§Œ ë„£ê±°ë‚˜ text+image(rectangular region)ë¥¼ ë„£ì–´ì„œ ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤

  ![image-20220904230127853](Dall-e-imgaes/image-20220904230127853.png)

  - ë¬¸ì¥ìœ¼ë¡œ ë„£ê±°ë‚˜, ë¬¸ì¥+ì´ë¯¸ì§€ë¥¼ ë„£ì„ ìˆ˜ ìˆìŒ
  - ì´ë¯¸ì§€ í† í°ì€ 1,024ê°œ ê¹Œì§€ ì…ë ¥ ê°€ëŠ¥
  - ë””ì½”ë”ëŠ” ê° 8,912ê°œ codebook vector ì¤‘ í™•ë¥ ê°’ì„ êµ¬í•´ì£¼ëŠ” ì—­í• 
  - latent codeë¥¼ ë‹¤ìŒ í† í°ì— ì¶”ê°€í•´ì„œ ë‹¤ìŒ ë¬¸ì¥ì´ ë¬´ì—‡ì¼ì§€ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ 1,024ê°œì˜ ì½”ë“œê°€ ìŒ“ì¼ ë•Œê¹Œì§€ ë°˜ë³µ -> ê·¸ í›„ í…ì„œë¡œ ë¬¶ì–´ì„œ dVAE ë””ì½”ë”ì— ë„£ì–´ì„œ ì´ë¯¸ì§€ ìƒì„±



### DALL-E í•™ìŠµ ê³¼ì •

- ì „ì²´ í•™ìŠµ ê³¼ì •ì€ joint likelihoodì— ëŒ€í•œ ELBO(Evidence Lower Bound)ë¥¼ maximizing í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ

![image-20220904230723160](Dall-e-imgaes/image-20220904230723160.png)





1. Stage 1  : Learning the Visual Codebook

   - ë¨¼ì € transformerë¥¼ ê³ ì •í•œ ìƒíƒœë¡œ **dVAE ì¸ì½”ë” *qÏ†* ì™€ dVAE ë””ì½”ë” *p*ğœƒ ë¥¼ í•™ìŠµ** (K= 8,192 codebook vectors)
     - ì´ë•Œ ì´ˆê¸° prior transformer ğ‘ğœ“ ëŠ” uniform categorical distributionìœ¼ë¡œ ì„¤ì •

   - <img src="C:/Users/yes47/AppData/Roaming/Typora/typora-user-images/image-20220903150249329.png" alt="image-20220903150249329" style="zoom: 67%;" />

     - ê³ ì–‘ì´ í„¸, ìƒì  ê¸€ì, ì¼ëŸ¬ìŠ¤íŠ¸ì—ì„œì˜ ì–‡ì€ ê¸€ìì™€ ê°™ì´ ë””í…Œì¼ì€ ë•Œë•Œë¡œ ì†ì‹¤ë˜ì§€ë§Œ, main featureë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì—¬ì „íˆ ì¸ì‹ì´ ê°€ëŠ¥í•¨. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ 8,192 í¬ê¸°ì˜ í° vocabulary sizeë¥¼ ì‚¬ìš©í•¨ (codebook vectorì˜ ê°œìˆ˜)

   - DALL-Eì—ì„œëŠ” discrete problemì„ **gumbel softmax relaxation**ì„ ì´ìš©í•´ í•´ê²°

   - ë‹¨ìˆœíˆ argmaxë¥¼ ì´ìš©í•´ codebook vector ì¤‘ì— (ê°€ì¥ ê°€ê¹Œìš´) í•˜ë‚˜ì˜ ì¸ë±ìŠ¤ë¥¼ êµ¬í•˜ê²Œ ë˜ë©´ gradientë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŒ

     ![image-20220904231841176](Dall-e-imgaes/image-20220904231841176.png)

     - (tau)*Ï„* ì˜ ê°’ì´ 0ì— ê°€ê¹Œì›Œì§ˆ ìˆ˜ë¡ hardí•œ distributionì´ ë§Œë“¤ì–´ì§. -> íƒ€ì´íŠ¸í•´ì§

   - ê²°ê³¼ì ìœ¼ë¡œ argmax ëŒ€ì‹ ì— ì´ë ‡ê²Œ ê³„ì‚°ëœ sampled latent vector zë¥¼ ì‚¬ìš©í•´ í•™ìŠµí•¨

     - The relaxation become tight as the temperature *Ï„* -> 0.

   

2. Stage two: Learning the Prior

   - ì´í›„ì— dVAE ì¸ì½”ë”  *qÏ†* ì™€ dVAE ë””ì½”ë” *pğœƒ* ë¥¼ ê³ ì •í•œ ìƒíƒœë¡œ the prior distribution (transformer)  ğ‘ğœ“ ë¥¼ í•™ìŠµ
     - 120ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ sparse transformer ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©
   - ì´ë¯¸ì§€ í† í°ì€ dVAE ì¸ì½”ë”ì˜ ê²°ê³¼ logitsì—ì„œë¶€í„° argmax samplingì„ ì§„í–‰í•˜ì—¬ ìƒì„±
   - ëª¨ë“  í…ìŠ¤íŠ¸ í† í°ì— ëŒ€í•˜ì—¬ í•­ìƒ attention í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ë‹¤ì–‘í•œ attention maskë“¤ì„ í™œìš©

   ![image-20220903151343155](Dall-e-imgaes/image-20220903151343155.png)



3. ê²°ê³¼ ì´ë¯¸ì§€ ë¶„ì„
   - í•™ìŠµì„ ë§ˆì¹œ ë’¤ì—ì„œ í•˜ë‚˜ì˜ textì— ëŒ€í•˜ì—¬ Nê°œì˜ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒ
   - ìƒì„±í•œ ë’¤ì—ì„œëŠ” ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ê³ ë¥´ê¸° ìœ„í•´ CLIP (OpenAI 2021)ì„ ì‚¬ìš©í•´ ì£¼ì–´ì§„ textì™€ kë²ˆì§¸ë¡œ similarityê°€ ë†’ì€ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŒ (í˜„ì¬ k=1)



4. ì„±ëŠ¥ ë¹„êµ

   - StackGAN, AttnGAN, DF-GAN, DM-GAN

   ![image-20220904232859883](Dall-e-imgaes/image-20220904232859883.png)



5. Zero-Shot Visual Reasoning : Image-to-Image Translation

   - ë§¤ìš° ë§ì€ ê°œìˆ˜ì˜ ë°ì´í„°ë¡œ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì—, íŠ¹ì • taskì— ëŒ€í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•˜ì—¬ ë°”ë¡œ ë™ì‘í•˜ë„ë¡ í•´ë„ ê½¤ ë†’ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ

   ![image-20220904233212417](Dall-e-imgaes/image-20220904233212417.png)



-----

Stage 1  : Learning the Visual Codebook

1.  ğœƒì™€ ğœ™ì— ëŒ€í•´ ELBë¥¼ Maximizing (Imageì— ëŒ€í•´ dVAE í•™ìŠµí•˜ëŠ” Step, Only Image)
   - Initial prior ğ‘ğœ“ : Codebook vector (K=8192)ì— ëŒ€í•œ Uniform categorical distribution ì„¤ì •
   -  ğ‘ğœ™ : 8192ê°œì˜ logitì— ì˜í•´ parameterize ë˜ëŠ” categorical distribution ì„¤ì •
     (dVAE Encoder output = 32 x 32, ê° Pointë§ˆë‹¤ 8192ê°œì˜ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸)
2. â†’ Optimizeê°€ ì–´ë ¤ì›€ (ğ‘ğœ“: Discrete distribution, Reparameterization gradient ì‚¬ìš© ë¶ˆê°€ëŠ¥)
   â†’ Gumbel-softmax relaxationì„ í†µí•´ í•´ê²° (ê¸°ëŒ€ê°’ ëŒ€ì²´ : ğ‘ğœ™â†’ ğ‘ğœ™
   Î¤ , Î¤ â†’ 0 ì¼ìˆ˜ë¡ Relaxation tight)
   â†’ Likelihood for ğ‘ğœƒ : Log-laplace distributionì„ í™œìš©í•˜ì—¬ Evaluate

3. Reparameterization gradient 
   - Gradient estimates computed via the Reparameterization trick
   -  Stochastic node = Stochastic + Deterministic (Backpropagation) : ë¯¸ë¶„ ê°€ëŠ¥, Continuous í•´ì•¼í•¨

4.  Gumbel-softmax relaxation
   - Gumbel-max : Categorical variableì„ Reparameterization 
   -  Gumbel-softmax : argmax termì„ softmaxë¡œ ê·¼ì‚¬ (ë¯¸ë¶„ ê°€ëŠ¥)
   -  Temperature ğœ : 0 â€“ One-hot, âˆ - Uniform distribution 



5. Annealing schedule for relaxation temperature and step size
   - Temperature ğœ ë¥¼ 1/16ìœ¼ë¡œ Annealing í•˜ë©´ Relaxed validation ELB â‰ˆ ì‹¤ì œ Validation ELB

6. Using 1x1 Convolutions at the end of Encoder & the beginning of Decoder
   - Relaxation ì£¼ë³€ì˜ Convolutionì—ì„œ Receptive fieldì˜ í¬ê¸°ë¥¼ ì¤„ì˜€ì„ ë•Œ, ì‹¤ì œ ELBë¡œ ë” ì˜ ì¼ë°˜í™”

7. Multiplying a small constant to the outgoing activations of Encoder & Decoder
   - ì´ˆê¸° í•™ìŠµ ë¶€ë¶„ì˜ ì•ˆì •ì„±ì„ ë³´ì¥í•´ ì¤„ ìˆ˜ ìˆìŒ 

8. KL Weight ğ›½ = 6.6ìœ¼ë¡œ ì„¤ì •í–ˆì„  ë•Œ, Codebookì˜ íš¨ìœ¨ì„ ë†’ì´ê³  Reconstruction Errorê°€ ì¤„ì—¬ ì¤Œ



Stage 2 : Learning the Prior

- ğœƒì™€ ğœ™ë¥¼ ê³ ì •ì‹œí‚¨ í›„ ğœ“ì— ëŒ€í•œ ELBë¥¼ Maximizing (Textì™€ Image tokenì˜  Prior distribution í•™ìŠµ)
  - Initial prior ğ‘ğœ“ : 130ì–µ ê°œì˜ Parameterë¥¼ ê°–ëŠ” Sparse transformerì— í•´ë‹¹
  - Text : Captionì„ ì†Œë¬¸ìí™” í•œ í›„ 16384ê°œì˜ Vocaë¥¼ ì‚¬ìš©í•˜ì—¬ BPE-encoding (ìµœëŒ€ 256 Tokens)
  -  Image : 32 x 32 = 1024 Tokenìœ¼ë¡œ Encoding (Voca sizeëŠ” 8192ê°œ)
    dVAE Encoder logitì—ì„œ Argmax samplingì„ í†µí•´ Image tokenì„ ì–»ìŒ
  -  ìµœì¢…ì ìœ¼ë¡œ Text & Image tokenì„ Concatenateí•˜ì—¬ Single stream of dataë¡œì„œ
    Autoagressiveí•˜ê²Œ Modeling



-  Transformer : Decoder-only model
  -  Text-to-text attention : Standard casual mask ì ìš©
  -  Image-to-image attention : Row/Col/Convolutional attention mask ì ìš© ê°€ëŠ¥





- Transformer Embedding Scheme

![image-20220903151359558](C:/Users/yes47/AppData/Roaming/Typora/typora-user-images/image-20220903151359558.png)

- GPU Efficiency

  - Mixed-Precision Training

    - GPU Memory íš¨ìœ¨ì„± í–¥ìƒì„ ìœ„í•´ 16-bit 
      precisionì„ ì‚¬ìš©
    - Standard loss scaling ëŒ€ì‹  Per-resblock
      gradient scaling ë°©ë²• ì´ìš©

    ![image-20220903151505240](C:/Users/yes47/AppData/Roaming/Typora/typora-user-images/image-20220903151505240.png)

  -  Distributed Optimization

    - 120ì–µ ê°œì˜ Parameter model ì€ 16-bit 
      precision ì‹œ 24GB ë©”ëª¨ë¦¬ ì°¨ì§€

    - PowerSGDë¥¼ ì´ìš©í•˜ì—¬ Gradient ì••ì¶•

      ![image-20220903151543186](C:/Users/yes47/AppData/Roaming/Typora/typora-user-images/image-20220903151543186.png)

-----

## CLIP

![image-20220905221308804](Dall-e-imgaes/image-20220905221308804.png)



![image-20220905221350899](Dall-e-imgaes/image-20220905221350899.png)

----

## CODE ver. pytorch

1. DALLE 2ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì€ 3ë‹¨ê³„ ê³¼ì •. CLIP í›ˆë ¨ì´ ê°€ì¥ ì¤‘ìš”
   - CLIP í›ˆë ¨ì‹œí‚¬ ë•Œ x-clip íŒ¨í‚¤ì§€ë¥¼ ì´ìš©

```python
import torch
from dalle2_pytorch import CLIP

clip = CLIP(
    dim_text = 512,
    dim_image = 512,
    dim_latent = 512,
    num_text_tokens = 49408,
    text_enc_depth = 1,
    text_seq_len = 256,
    text_heads = 8,
    visual_enc_depth = 1,
    visual_image_size = 256,
    visual_patch_size = 32,
    visual_heads = 8,
    use_all_token_embeds = True,            # whether to use fine-grained contrastive learning (FILIP)
    decoupled_contrastive_learning = True,  # use decoupled contrastive learning (DCL) objective function, removing positive pairs from the denominator of the InfoNCE loss (CLOOB + DCL)
    extra_latent_projection = True,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)
    use_visual_ssl = True,                  # whether to do self supervised learning on images
    visual_ssl_type = 'simclr',             # can be either 'simclr' or 'simsiam', depending on using DeCLIP or SLIP
    use_mlm = False,                        # use masked language learning (MLM) on text (DeCLIP)
    text_ssl_loss_weight = 0.05,            # weight for text MLM loss
    image_ssl_loss_weight = 0.05            # weight for image self-supervised learning loss
).cuda()

# mock data

text = torch.randint(0, 49408, (4, 256)).cuda()
images = torch.randn(4, 3, 256, 256).cuda()

# train

loss = clip(
    text,
    images,
    return_loss = True              # needs to be set to True to return contrastive loss
)

loss.backward()

# do the above with as many texts and images as possible in a loop
```







---



## Summary

- Autoregressive transformer ê¸°ë°˜ Text-to-image generation taskë¥¼ ìœ„í•œ ì ‘ê·¼ë²• ì œì•ˆ
- 120ì–µ ê°œì˜ Parameterë¡œ ì´ë£¨ì–´ì§„ Large scale Modelë¡œì¨ GPT-3ì˜ Image í™•ì¥ í˜•íƒœ
- Zero-shot performanceì™€ Single generative model ë“±ì˜ ê´€ì ì—ì„œ í›Œë¥­í•œ ìˆ˜ì¤€ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ì„
- Captionì— ì‚¬ë¬¼ì´ ë§ì´ í¬í•¨ë˜ë©´(ë³µì¡í• ìˆ˜ë¡) ì¡°í•©í•´ì„œ ê·¸ë¦¬ì§€ ì•Šê³ , ì‚¬ë¬¼ì„ ê·¸ëŒ€ë¡œ ê·¸ë¦¬ë ¤ëŠ” í•œê³„ì ì„ ì§€ë‹˜
  (ex. â€œíŒŒë€ ë”¸ê¸° ì´ë¯¸ì§€ê°€ ìˆëŠ” ìŠ¤í…Œì¸ë“œê¸€ë¼ìŠ¤ ì°½â€ ì´ë¼ëŠ” Captionì´ ë“¤ì–´ì˜¤ë©´, ì¡°í•©í•œ ê²°ê³¼ê°€ ì•„ë‹Œ ê°ê°ì˜
  ì‚¬ë¬¼ì„ ê·¸ëŒ€ë¡œ ê·¸ë¦° ê²°ê³¼ë¥¼ ë³´ì„)
- Text2Art : VQGAN+CLIP