# 딥러닝 DAY1

수업 정리한 내용 복습



## 인공지능 개요

- 인공지능
  - 사고나 학습 등 인간이 가진 지적 능력을 컴퓨터를 통해 구현하는 기술
- 머신러닝
  - 컴퓨터가 데이터를 스스로 학습하여 인공지능의 성능을 향상 시키는 기술 방법
- 딥러닝
  - 인공신경망 이론을 기반으로 복잡한 비선형 문제를 기계가 스스로 학습 해결

<br>

### 학습
- 기억(Memorization)하고 적응(Adaptation)하고, 이를 일반화(Generalization) 하는 것
- 어떤 데이터를 경험을 토대로 패턴은 이거다 계속해서 알려주게 됨

<br>

### 기계학습
- 명시적인 프로그래밍을 하지 않고도 학습하는 능력을 갖춤
작업(T, Task)의 성능(P, Measurement)을 측정했을 때 경험(E, Data)으로 성능이 향상됐다면, 이 컴퓨터 프로그램은 경험으로부터
학습했다고 볼 수 있다.
- 주가 예측, 의료, 추천 시스템, 음성 인식, 자동 주행 등에 쓰임

<br>

### Supervised Learning (지도학습)
- 특징
  - 라벨이 있다 (y값이 있다)
  - class = target = y data = label = 정답 


- 종류
  - Classification > 출력 : 클래스
    - knn : 새로운 데이터가 어떤 그룹에 속하는지 분류하기 위해 그 데이터가 가장 가까이 있는 학습 데이터의 그룹을 알아보는 모델
    - SVM : 데이터를 분리해 가장 멀리 분리된 경우가 높은 신뢰도를 준다는 모델
    - 의사 결정 트리 : 질문과 답을 반복적으로 이등분하는 방식으로 찾는 모델, 신뢰도를 높이기 위해서 엔트로피를 통해 정보의 가치가 높은것을 식별함
    - 인공신경망
  - Regression > 출력 : 숫자(실수)
    - 입력에 따라 실수를 출력 (정답이 실수)
    - 데이터를 가장 잘 표현할 수 있는 선을 찾는 것. 
    - 입력 데이터의 상관관계를 잘 파악하여 출력을 예측.
    - Linear Regression, polynomial regression, GAM, GLM ..

<br>


### Unsupervised Learning (비지도학습)
- 특징
  - 레이블(정답)이 없다


- 종류
  - Clustering
    - 데이터들을 그룹으로 나눔
- 데이터간의 서로 가깝거나 비슷한 것끼리 그룹화 함.
    - Anomaly Detection

<br>

<br>


## 딥러닝
- 장점
  - 특정 분야에서는 드라마틱한 성능 향상을 꽤 할 수 있다.
  - Feature selectino 에 큰 공을 들이지 않더라도, 모델이 end to end로 학습하고
  - 선별하는 경우가 많다
  - 고도의 수학적 지식이나 프로그래밍 능력을 요구하지 않는다
  - 오픈소스 알고리즘이 풍부하여 저렴하고 빠르게 개발할 수 있다.

- 단점
  - 상당히 많은 Label 데이터가 필요할 때가 많다
  - 작은 양의 데이터로는 under fit 되는 경우가 ml의 경우보다 더 많다
  - gpu나 컴퓨터 성능이 좋아야함...

- Framework 종류
  - Tensorflow ( Google )
  - Keras (Google )
  - Pytorch ( Facebook )
  - (참고)서비스를 배포할 때 tensorflow를 알아야 배포가 가능


<br>

<br>

## Tensorflow
- Tensor
  - 행렬로 표현할 수 있는 2차원 형태의 배열을 높은 차원으로 확장한 다차원 배열을 말함

- Flow
  - 데이터 흐름 그래프(Dataflow Graph)에 따라 계산
  - 데이터 흐름 그래프는 노드(node)와 엣지(Edge)로 구성
  - 각 노드들을 독립변수로 지정하여 학습.

<br>

### 사칙연산 프로그램
```python
### tf를 통해 변수를 선언하고 ((4*2)-(1+2)) - 5 연산

## 상수 선언하기
x1 = tf.constant(4)
x2 = tf.constant(2)
x3 = tf.constant(1)
x4 = tf.constant(5)

## 연산

result1 = tf.subtract(tf.multiply(x1,x2), tf.add(x3,x2))
result = tf.subtract(result1,x4)

## 출력
print("result = {}".format(result))
```
> `tf.multiply()` : 요소 곱
> `tf.add()` : 더하기
> `tf.subtract` : 빼기

<br>


### 다차원 데이터 실습 (행렬 내적 곱)
```PYTHON
import tensorflow as tf

## matrix 선언 하기.
matrix_A = tf.constant([[2,2,4],
                        [1,1,6],
                        [1,3,8]])
matrix_B = tf.constant([[4,3,3],
                        [2,1,6],
                        [1,2,8]])

## matrix 연산하기.
result = tf.matmul(matrix_A, matrix_B)   # 내적 곱
# result = tf.multiply(matrix_A, matrix_B)   # 요소 곱

## 결과
print("result = {}".format(result))
```
> `tf.matmul()` : 내적 곱

<br>

<br>


## Perceptron
### Single neuron model : perceptron

- **perceptron은 layer 한 층이다. 노드 하나를 의미한다**
- **x * w +b = y**
  - w : 가중치의 벡터들, b y절편
- x라는 input이 들어 오고 w라는 다른 강도(가중치)를 곱해서 노드에 weight sum. activation function을 거친 다음에 y를 도출
- y와 정답 y의 오차를 구하게 되는데, 그 **오차를 줄이는게 목표**

$$
\hat{y}(x) =\begin{cases}1 & w^Tx>0\\ -1 & otherwise\end{cases} 
\\
\sum_{i=0}^n w_ix_i = w^Tx
$$

<br>



```python
import tensorflow as tf

## data 선언
x_data =[[1.],[2.],[3.],[4.]]
y_data =[[1.],[3.],[5.],[7.]]

## 평균 0, 분산 1의 파라미터의 정규분포로 부터 값을 가져옴.
# 학습을 통해 업데이트가 되어 변화되는 모델의 파라미터인 w,b를 의미한다.
W= tf.Variable(tf.random.normal((1,1), mean=0, stddev=1))  
b= tf.Variable(tf.random.normal((1,1), mean=0, stddev=1)) 
print("W : ", W)
print("b : ", b)
```

> W :  <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[1.3353487]], dtype=float32)>
>
> b :  <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[-0.9132107]], dtype=float32)>

```python
for j in range(len(x_data)):
    ## data * weight 작성
    WX = tf.matmul([x_data[j]],W) # perceptron 모델 

    ## bias add 작성
    y_hat = tf.add(WX, b)

    ## W와 b로 예측 하기
    print("y_data: , ",y_data[j], "prediction : ", y_hat)
```

> ```
> y_data: ,  [1.0] prediction :  tf.Tensor([[0.70487463]], shape=(1, 1), dtype=float32)
> y_data: ,  [3.0] prediction :  tf.Tensor([[0.8124287]], shape=(1, 1), dtype=float32)
> y_data: ,  [5.0] prediction :  tf.Tensor([[0.91998285]], shape=(1, 1), dtype=float32)
> y_data: ,  [7.0] prediction :  tf.Tensor([[1.027537]], shape=(1, 1), dtype=float32)
> ```
>
> `[x_data[j]]` -> 텐서로 만들기 위해
>
> **딥러닝은 텐서(행렬)연산을 수행한다.**

> 내가 한 방식
>
> ```python
> WX = tf.multiply(W,x_data[j]) # perceptron 모델 
> ```
>
> 강사님 방식
>
> ```python
> WX = tf.matmul([x_data[j]],W) # perceptron 모델 
> ```
>
> 텐서 연산을 수행하기 때문에 [ ] 해주고 `tf.matmul` 내적 곱하는 방식

<br>

<br>

### Single neuron model :perceptron learning

- $$
  E(w) \equiv  \frac{1}{2} \sum_{d \in D}(y_d - \hat{y}_d)^2
  $$

- **y_d의 의미?**

  - **실제값, target 값, 목표값**

<br>

- **y_hat의 의미는 ?**

  - **예측값**

  - $$
    \hat{y} = w_0 + w_1x_1 + ... + w_nx_n\\
    w_i \leftarrow  w_i + \Delta{w_i}
    $$

  - 1. 각각의 가중치에 대해 임의의 값으로 설정
    2. 잘 될 때까지 조금씩 값을 변경

<br>

- **y_d - y_hat_d = 정답과 신경망이 계산한 (추측한) 값의 차이**
  - 이 식이 최소가 되도록 하는 그때의 w값을 구하면 됨

<br>

- $$
  (WX-Y)^2
  \\ =W^2X^2 - 2WXY + Y^2
  \\ =AW^2 - BW + C
  \\ cost(error)
  $$

  - wx = 모델이 예측한 값

  - -> x값 : w(weight), y값 : E (cost)

  - **Cost가 최소가 되는 W를 찾는 것이 목표**

  - $$
    W = W - \frac{\partial}{\partial W}cost(W)
    $$

  - 다음 W = 현재 W – 현재W의 cost를 미분

  - 구해야 할 w값이 한 개 일 때는 2차 그래프가 되는데

    구해야 할 w값이 두 개 일 때 초평면이 됨

<br>

- 처음 설정한 w 값이 최소 값의 오른쪽이었다면 기존 값에서 조금 빼주어야 함

  - $$
    \Delta w_i = 음수\ for\ w^a
    $$

  - 기울기를 구하면 반대 방향으로 가는 것이 목표기 때문에 기울기에 –를 붙여야 함

- 처음 설정한 w 값이 최소 값의 왼쪽이었다면 기존 값에서 조금 더해주어야 함

  - $$
    \Delta w_i = 양수\ for\ w^b
    $$

<br>

- 학습률(learning rate)
  - **learning rate를 아주작은 값이 아닌 큰값을 곱해주게 되면 어떤 현상이 발생하게 될까요?**
    - 러닝 레이트를 크게 한다면 최솟값을 지나칠 수 있다. 발산을 해버릴 수 있다
    - 그래서 작은 값을 곱해주면서 최솟값에 다가갈 수 있도록 한다

<br>

- 경사하강법 정리

- $$
  \Delta w_0 = - \eta \frac{\partial E}{\partial w_0} = \eta \sum_d (y_d - \hat{y}_d)
  $$

- 

<br>

### Single neuron model : perception linear regression

- 


```python
import tensorflow as tf

# [1]
## data 선언
x_data =[[1.],[2.],[3.],[4.]]
y_data =[[1.],[3.],[5.],[7.]]

# [2]
## 평균 0, 분산 1의 파라미터의 정규분포로 부터 값을 가져옴.
# 학습을 통해 업데이트가 되어 변화되는 모델의 파라미터인 w,b를 의미한다.
W=tf.Variable(tf.random.normal((1,1),mean=0, stddev=1.0))
b=tf.Variable(tf.random.normal((1,1),mean=0, stddev=1.0))

# [3]
lr=tf.constant(0.0001)

# [4]
for i in range(2000):  ## 에폭
    total_error = 0

    # [5]
    for j in range(len(x_data)): ## 배치 1
        # [6]
        ## data * weight
        WX =tf.matmul([x_data[j]], W)  

        ## bias add
        y_hat = tf.add(WX, b) # perceptron

        # [7]
        ## 정답인 Y와 출력값의 error 계산
        error = tf.subtract(y_data[j], y_hat) # (prediction - true)^2

        # [8]
        ## 경사하강법으로 W와 b 업데이트.
        ## 도함수 구하기
        diff_W = tf.multiply(x_data[j], error ) # error*x의 합
        diff_b = error

        ##  업데이트할 만큼 러닝레이트 곱
        diff_W = tf.multiply(lr, diff_W) 
        diff_b = tf.multiply(lr, diff_b) # lr * error

        # [9]
        ## w, b 업데이트
        W = tf.add(W, diff_W )# w + lr * x * error
        b = tf.add(b, diff_b) # b + lr * error
        #######

        # [10]
        ## 토탈 에러.
        visual_error = tf.square(tf.subtract(y_hat, y_data[j]))
        total_error = total_error + visual_error

    ## 모든 데이터에 따른 error 값
    print("epoch: ", i, "error : ", total_error/len(x_data))
```
> [1]
> Data 선언 (2차원) -> X : (4,1), y: (4,1), W : (2,1)
> ```python
> import numpy as np
>print(np.array(x_data).shape)
>print(np.array(y_data).shape)
>print(W.shape)
>```

> [2]
> - Perceptron의 W와 b 변수를 선언 

> [3]
> - 경사하강법으로 w와 b를 업데이트할 값을 종할 learning rate

> [4]
> - 전체 데이터를 다 학습했을 경우 Epoch가 1이 올라감
> - 즉, 전체 데이터를 2000번 학습하도록 되어 있음
> - Epoch : 2000

> [5]
> - for문으로 동작, 4개의 data set이 모두 학습되면 epoch이 1 올라가고, 다시 data set을 처음부터 입력 받으며 학습

> [6]
> - WX = [ [1,]] * [ [w_init]] / [ [2,]] * [ [w_update]]
> - WX + b = y_hat 

> [7]
>
> - wx + b = y - 1 (정답과 빼서 error)

> [8]
>
> - 경사하강법으로 구한 w, b 변화량 * lr
>
> - $$
>   \Delta w_i = -\eta \frac{\partial E}{\partial w_i} = \eta \sum_d (y_d - \hat{y}_d)(x_{d,i})\\
>   \Delta w_0 = -\eta \frac{\partial E}{\partial w_0} = \eta \sum_d (y_d - \hat{y}_d)
>   $$

> [9]
>
> W, b 업데이트
> $$
> W_i = W_i + \Delta W_i
> \\
> W_0 = W_0 + \Delta W_0
> $$



- **학습이 잘 되었다면, error 몇으로 줄어드는게 가장 이상적인 error일까요?**
  - 0으로 수렴되는 것이 이상적

- **학습이 잘 되고 있지 않다면**
  - 값이 0으로 수렴되지 않거나 loss가 줄어들지 않은 경우
  - 줄어들다가 크게 증가 > 오버 피팅 될 경우임



- **perceptron의 입력변수 갯수 ==?, 레이의 갯수 ==?**
  - perceptron은 입력변수가 1개이고, layer층이 1개이다.
  - input 1개, input_dim은 1 perceptron

<br>

<br>

### Single neuron model : perception classification

```python
import tensorflow as tf

## data 선언
x_data =[[1.],[2.],[3.],[4.]]
y_data =[[1.],[3.],[5.],[7.]]

test_data=[[9.]]

# [1]
## tf.keras를 활용한 perceptron 모델 구현.
model = tf.keras.Sequential()   # 모델 만들기 위해 sequential 메서드를 선언, 이를 통해 모델을 만들 수 있다.
model.add(tf.keras.layers.Dense(1, input_dim=1)) #( Code ) # 선언된 모델에 add를 통해 쌓아감. , 현재는 입력 변수 갯수 1, perceptron 1개.
model.summary()  ## 설계한 모델 프린트  

# [2]
# 모델 loss, 학습 방법 결정하기
optimizer=tf.keras.optimizers.SGD(lr=0.002) ### 경사 하강법으로 global min 에 찾아가는 최적화 방법 선언.
loss=tf.keras.losses.mse  ## 예측값 과 정답의 오차값 정의. mse는 mean squre error로 (예측값 - 정답)^2 를 의미
metrics=tf.keras.metrics.mae     ### 학습하면서 평가할 메트릭스 선언 mae는 mean_absolute_error |예측값 - 정답| 를 의미

# [3]
# 모델 컴파일하기 학습방법을 정의
model.compile(loss=loss, optimizer=optimizer, metrics=[metrics])

# 모델 동작하기
model.fit(x_data, y_data, epochs=2000, batch_size=4)
```

> [1]
>
> tf.kears.layers.Dense = wx+b 
>
> > for 문으로 x가 하나씩 들어가서 output 1개
> >
> > x -> perceptron -> y_hat
> >
> > input 1개, input_dim은 1 perceptron
>
> dense층 (None, 1)
>
> > None : 아무것도 지정하지 않아서 default 값
> >
> > parm : 학습 가능 한 parameter
> >
> > y = wx + b
> >
> > w 1 x 1 + b 1 = 1 * 1 + 1 = 2

>[2]
>
>도함수 > optimizer
>
>> optimizer : SGD > 경사 하강법으로 global min 에 찾아가는 최적화 방법
>
>loss > mse
>
>>예측값 – 정답의 차이를 구해서 제곱을 한 것을 loss의 방법으로 하겠다
>>
>>mean square error 에러를 최소화하게 만들어 준다
>>
>>평가지표는 mae |예측값 – 정답|

> [3]
>
> input_layer에서 shape = 데이터에 맞게 직접 지정, batch_size = None(기본값)
>
> > Batch_size는 학습을 할때 마다 바뀔 수 있고 학습 완료 후 여러개의 데이터를 동시에 넣지 않고 하나의 데이터씩 넣어서 테스트 하는경우가 있음.
> >
> > 즉, Batch_size는 유동적이어야 함



### 에폭(epoch)과 배치(batch) 차이

- 에폭 (epoch)
  - 딥러닝 epoch는 전체 트레이닝 셋이 신경망 통과한 횟수
- 배치 (batch)
  - 전체 트레이닝 데이터 셋을 여러 작은 그룹을 나누었을 때 batch size는 하나의 소그룹에 속하는 데이터 수를 의미

- iteration
  - 1-epoch를 마치는 데 필요한 미니배치 개수

![epoch02](DL_0721-imgaes/epoch02.jpg)

> 이미지 출처 : https://losskatsu.github.io/machine-learning/epoch-batch/#3-epoch%EC%9D%98-%EC%9D%98%EB%AF%B8

> for 에폭
>
> ​	for 배치