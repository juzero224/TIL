# DL DAY6 TIL

멀캠 수업 내용 정리





## RNN

- 순환 신경망
  - 각 층의 결괏값이 출력층을 향하면서도 동시에 현재 층의 다음 계산에 사용



- 과거의 정보를 이용하여 현재의 상태 업데이트

- 외부 입력과 자신의 이전 상태를 입력 받아 자신의 상태 갱신

- $$
  h_t = f_w( h_{t-1}, x_t)
  $$





- SimpleRNN의 vanishing gradient 문제 해결
  - relu 사용 >> weight들이 폭발할 위험성이 있어서 행렬값을 잘 설정하는게 중요함
  - LSTM, GRU 레이어 사용





### LSTM

- 장기 기억, 단기 기억을 조작하는 두 개의 셀 존재
- input gate, forget gate, output gate
- 'Cell state'를 통해 이전 정보를 계속해서 사용해서 vanishing gradient 문제 방지





#### Forget gate

- <u>장기기억 중 불필요한 정보</u> 삭제

- 장기 기억으로 넘어갈 데이터를 Sigmoid 함수를 사용해서 판단

- ```
  h = 단기 기억
  c = 장기 기억
  ```



#### Input gate

- <u>장기 기억 후보군</u> 계산

1. 과거 단기 기억과 현재 입력 값으로부터 Sigmoid 함수를 사용해서 어떤 정보를 업데이트할지 결정
2. 과거 단기 기억과 현재 입력 값으로부터 tanh 함수를 사용해서 새로운 정보를 만듦



#### Cell state

- <u>'forget gate'를 통해 걸러진 정보</u>와 <u>장기기억 후보 값 x input 게이트를 곱해 나온 중요한 정보</u>를 합해서  Cell state에 업데이트
- forget gate + input gate 종합



#### Output gate

- hidden layer에 업데이트할 데이터 정하기

- 과거의 단기 기억과 현재 입력 값으로 Output 게이트 계산
- tanh 함수를 통과한 장기기억 'Cell state'에 Output 게이트 곱해서 hidden state 업데이트





### GRU

- 게이트 순환 유닛

> :question: GRU가 LSTM보다 학습할 데이터가 적을까요? 많을까요? 적다고 생각하면 왜 그렇게 생각하나요?
>
> - GRU는 중복값을 제거해서 연산량이 적음 >> 학습할 파라미터가 적다

- update gate, reset gate





#### Update gate

- 현재 기억을 만들기 위한 과거 기억과 현재 입력의 비율

- $$
  z_t= \sigma (W_z · [h_{t-1}, x_t])
  $$

- z_t = 과거 기억과 새로운 입력을 모두 고려해서, <u>과거의 기억을 얼마나 버릴지</u> 정한 비율



#### Reset gate

- 재조정 비율 - 과거 기억을 재조정하는 비율

- $$
  r_t = \sigma (W_r · [h_{t-1},x_t])
  $$

- r_t : 새로운 정보를 만들기 위해서 과거의 기억을 얼마나 반영할지 정한 비율



#### Memory Candidate

- 현재 기억 후보 - 현재 입력과 재조정된 과거 기억 조합

- $$
  \tilde{h_t} = tanh(W · [r_t * h_{t-1}, x_t])
  $$

- `tanh` 사용





#### Final Memory

- 현재 기억 - 갱신 비율에 따른 과거 기억 + 현재 기억 후보 조합

- $$
  ht = (1-z_t) * h_{t-1} + z_t * \tilde{h_t} 
  $$







---



### RNN Regression

- Time step = input length = sequence
- time step : 이전의 데이터를 얼마나 반영할 것인지 정하는 것



`tf.keras.preprocessing.sequence.TimeseriesGenerator`



```python
seq_length=2
x_data_dim=4
batch_size=100
min_max_normalization_flag=True
```

> :question: seq_length가 무엇일까요? 
>
> - 현재의 데이터를 기준으로 과거의 데이터 사용 개수 = time step?

> x_data_dim : 데이터 특성 개수?





```python
model.add(simpleRNN(32 # 셀 숫자
                    input_shape(batch size, time step, input_dim)
                    activation = tanh
                    return_sequences = True
                    stateful = True
                   ))
```

> return_sequence = False : 마지막 hidden state의 결과값만 출력
>
> return_sequence = True : 이전 hidden state의 모든 시점 다 출력

> stateful = False : 마지막 상태가 다음 샘플 학습에 들어가지 않음
>
> stateful = True : 학습 샘플의 마지막 상태가 다음 학습 시에 입력됨



```python
input_Layer = tf.keras.layers.Input(shape=(seq_length, x_data_dim))
x = tf.keras.layers.GRU(50, activation='tanh', return_sequences=True)(input_Layer)  # RNN 층, units=20, activation='tanh'
# default는 return_sequence가 False >> True로 바꿔줘야 함
x = tf.keras.layers.GRU(50, activation='tanh', return_sequences=True)(input_Layer)  # 세층으로 쌓을 때 
x = tf.keras.layers.GRU(50, activation='tanh')(x)  
# 마지막 state는 return_sequence X 
x = tf.keras.layers.Dense(100, activation='relu')(x) # Dense 층, units=20, activation='relu'
x = tf.keras.layers.Dense(50, activation='relu')(x)  # Dense 층, units=10, activation='relu'
Out_Layer=tf.keras.layers.Dense(1,activation=None)(x)  # 예측 분류기
model = tf.keras.Model(inputs=[input_Layer], outputs=[Out_Layer])
model.summary()
```

> :question: 2개의 레이어를 stack으로 쌓을 때, 어떤 옵션을 어디 레이어에다가 추가해줘야할까요?
>
> - 첫번째 layer에 return_sequences=True
> - 이전 state를 다음 layer에 넘겨주어야 함
> - 3개 레이어를 stack을 쌓을 때 >> 첫번째, 두번째 layer에 return_sequences = True





----



## 토큰화

- 토큰 : 문법적으로 더 이상 나눌 수 없는 언어 요소
- 토큰화 : 이를 수행하는 작업
  1. 공백 기반 : 나는 / 밥을 / 먹었다
  2. 형태소 기반 : 나/는/밥/을/먹었/다
- `tensorflow.keras.preprocessing.text`





```python
tokenizer = Tokenizer(num_words = 10,        # 빈도수 높은 순
                      oov_token = '<OOV>')   # 없는 단어임을 알려줌
tokenizer.fit_on_texts(texts)    # 리스트로 바꿈

# index 확인
tokenizer.word_index  # {'<OOV>': 1, 'are': 3, 'best': 5, 'nice': 6, 'the': 4, 'you': 2}

# 텍스트 데이터를 정수 인덱스 형태로 변환
sequences = tokenizer.texts_to_sequences(texts)  # 정수 인덱스로 변환
```

> 1. `Tokenizer()` 로 최대 단어 개수 지정
> 2. 만약 문장에 포함되지 않은 단어 존재 >> `oov_token = <OOV>` (out of vocabulary) 단어로 대체
>    - 인덱스에서 `<oov>` 가 숫자 1로 변환한 것을 볼 수 있음
> 3. `texts_to_sequences()` : 토큰화를 통해 부여된 인덱스로 단어 변환





- 텍스트 데이터를 사용하는 경우엔 >> `Embedding Layer`사용 :star:
  - Embedding 층을 사용하기 위해서는 sequence 데이터 길이가 동일해야 함
  - => `pad_sequences()` 사용



- pad_sequences() : 데이터의 길이가 전부 동일하도록 조정
  - 짧은 경우 0으로 채움
  - 긴 경우 잘라냄

```python
X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)
```



```python
model = Sequential()
# 이 층은 모델의 제일 첫 번째 층으로만 사용할 수 있음
model.add(Embedding(vocab_size, embedding_dim))
model.add(GRU(hidden_units))
model.add(Dense(1, activation='sigmoid'))
```

- embedding 층은 모델의 첫 번째 층으로만 사용할 수 있음
- 