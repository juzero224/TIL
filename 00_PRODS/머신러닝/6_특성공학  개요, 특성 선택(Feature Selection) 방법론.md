# 6_특성공학 : 개요, 특성 선택(Feature Selection) 방법론

- 특성공학
  - 머신러닝 알고리즘에 사용되는 입력데이터에 해당하는 특성변수들에 대한 처리



- 특성 선택
  - 전체 특성변수 중 최적의 조합을 선택하는 문제



- 특성 추출
  -  특성변수들을 적절하게 조합하여 새로운 특성 변수를 만드는 문제 



- Filter 방식
  - 1:1 비교, 모델 훈련하지 않음, 속도가 빠름
- Wrapper 방식
  - 다:1 비교, 상호작용 반영 모델 훈련함
- Embedded 방식



## 특성 공학

### 특성공간 차원축소의 필요성

- 모델의 해석력 향상
  -  차원축소를 시켜서 서로 독립적인 정보를 반영한 소수의 특성변수를 이용해서 모델링을 한다면 정보를 많이 손실하지 않는 방향에서 모델의 해석력이 훨씬 더 좋아질 것
  - 변수가 작기 때문에 연관관계를 설명하기가 훨씬 더 좋아짐
- 모델 훈련시간의 단축
- 차원의 저주 방지
  - 고차원의 공간에서 sparse (데이터 간의 빈 공간)한 경우가 있어서 파악하기 어려워짐
- 과적합 (overfitting)에 의한 일반화 오차를 줄여 성능 향상



- 특성공학 방법론은 크게 특성 선택(feature selection) 방법과 특성 추출(feature extraction) 방법으로 구분할 수 있음



## 특성 선택 (Feature selection) 방법론

### 특성 선택 (feature selection)

- 주어진 특성 변수들 가운데 가장 좋은 특성변수의 조합만 선택
- 불필요한 특성 변수를 제거함
- Filtering, Wrapper, Embedded 방식으로 분류할 수 있음



![image-20221028220025591](C:/Users/yes47/AppData/Roaming/Typora/typora-user-images/image-20221028220025591.png)

- Filter : 특성변수 중에 y를 설명하는 데 중요한 순서대로 ranking을 매긴 후 순위가 높은 일부만 선택
- Wrapper : 전체의 특성 데이터가 있는데 일부 set을 후보로 데리고 온 후 우리 모델에 적합을 먼저 해봄 -> 다른 후보군을 선택해서 적합 -> 이 과정을 반복을 하면서 모델의 적합도 측면이나 어떤 평가 기준을 이용해서 어떤 변수의 조합이 최고인가 판단
- Embedded : 어떤 특성 변수를 선택할 지 모델 자체가 선택 



### Filter 방식 : 각 특성변수를 독립적인 평가함수로 평가함

- 각 특성변수 X_i와 목표변수(Y)와의 연관성을 측정한 뒤, 목표변수를 잘 설명할 수 있는 특성 변수만을 선택하는 방식
- X_i와 Y의 **1:1 관계**로만 연관성을 판단
- 연관성 파악을 위해 **t-test, chi-square test, information gain** 등의 지표가 활용됨





### Wrapper 방식 : 학습 알고리즘을 이용

- 다양한 특성 변수의 조합에 대해 목표변수를 예측하기 위한 **알고리즘을 훈련**하고, **cross-validation 등의 방법**으로 훈련된 모델의 예측력을 평가함. 그 결과를 비교하여 최적화된 특성변수의 조합을 찾는 방법
- 특성변수의 조합이 바뀔 때마다 모델을 학습함
- 특성변수에 **중복된 정보가 많은 경우** 이를 효과적으로 제거함
- 대표적인 방법으로는 순차탐색인 **forward selection, backward selection, stepwise selection** 등이 있음

<br>

- ex. x1, x2, x3 변수가 있을 때,

  x1 ~ y, x2 ~ y, x3 ~ y, x1x2 ~ y, x1x3 ~ y, x2x3 ~ y, x1x2x3 ~ y

  2^k-1개

<br>

- 시간이 많이 걸리고 비용이 많이 듦 => 해결방법
  - forward selection : 중요한 애들부터 하나씩 순차적으로 모형에 포함을 해 가면서 더이상 중요한 애가 없으면 멈추는 방식
  - backward selection : 전부 다 포함한 모형에서 시작해서 중요하지 않은 애들부터 하나씩 제거
  - stepwise selection : forward 와 backward를 동시에 진행. 중요한 게 있으면 선택, 중요하지 않은 변수는 제거 

<br>

- filter 방식과의 차이점
  - filter는 1:1로 연관성 판단 / wrapper는 변수들의 조합과 y를 평가. 다대일 개념
  - filter는 모델링을 하지 않음 / wrapper는 조합이 바뀔 때마다 모델을 한번씩 다 fitting을 해서 Best를 찾아냄 (모델 학습 O)



![image-20221028221337337](C:/Users/yes47/AppData/Roaming/Typora/typora-user-images/image-20221028221337337.png)





### Embedded 방식 : 학습 알고리즘 자체에 feature selection 을 포함하는 경우

- Wrapper 방식은 모든 특성변수 조합에 대한 학습을 마친 결과를 비교하는데 비해, Embedded 방식은 **학습 과정에서 최적화된 변수를 선택한다는** 점에서 차이가 있음
- 대표적인 방법으로는 특성변수에 규제를 가하는 방식인 **Ridge, Lasso, Elastic net** 등이 있음





## 문제

**Q1. 다음 중 특징 선택(feature selection)과 관련이 없는 설명은 무엇인가?**

1. **기존의 특성변수를 결합하여 새로운 변수를 생성한다.** 

2. 전체 특성변수들 가운데 가장 좋은 특성변수의 조합을 찾고자 한다.

3. 특성변수들 가운데 중복된 정보를 제거하는 효과가 있다.

4. 특성변수의 조합을 찾기 위해 순차탐색법을 사용하기도 한다.



- 기존의 특성변수를 결합하여 새로운 변수를 생성하는 방식은 특징 추출(feature extraction)에 해당한다.



**Q2. 특징 선택(feature selection) 기법 중 Filter 방식의 특징으로 볼 수 없는 것은?**

1. 선택된 특성변수는 목표변수의 예측에 중요한 변수로 볼 수 있다.

2. **선택된 입력변수 간에는 거의 상관관계가 존재하지 않게 된다.** 

3. 계산 비용이 적고, 속도가 빠른 방식이다.

4. 대부분의 기법들이 특성변수와 출력변수 사이의 선형성을 기반으로 선택한다.



- Filter 방식은 특성변수와 목표변수 간의 일대일 관계로 중요성 여부를 판단하므로, 선택된 특성변수들 간의 상관성은 고려하지 않는다. 따라서 **서로 상관관계가 높은 중복된 특성변수들이 함께 선택될 수 있다는 것이 단점**이다.



**Q3. 이는 특징 선택 방법의 하나로, 주어진 특성 변수들 중 최적의 조합을 찾기 위하여 다양한 특성변수의 조합에 대하여 매번 목표변수를 예측하기 위한 모델을 훈련하는 방식을 말한다. 따라서 여러 차례 모델을 학습해야 해서 계산비용이 크지만, 특성 변수 간 상호작용을 고려한 변수선택이 가능하다는 것이 장점이다. 이는 어떤 특징 선택 방법에 대한 설명인가?**

1. Filter 방식

2. **Wrapper 방식**

3. Embedded 방식

4. Extraction 방식



- Wrapper 방식은 특성변수 조합별로 모델을 적합한 뒤 예측 성능을 비교하는 방식으로 최적의 특성 변수의 조합을 찾아내는 방식을 말하며, 순차 탐색법 등이 포함된다.